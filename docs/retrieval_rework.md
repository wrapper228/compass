## Текущее состояние retrieval

- **Ингест**: `process_zip` нарезает файлы на параграфы (~1200 символов), выполняет MinHash-дедупликацию, для каждого чанка синхронно получает эмбеддинг через внешний API и складывает точки в Qdrant (`chunks`, косинус).  
- **Метаданные**: в payload точки попадают `job_id`, путь, порядковый индекс чанка, сырой текст. Жёстких ограничений на токены нет, линии и версии не фиксируются.  
- **Онлайн-поиск**: `retrieve_for_text` получает эмбеддинг запроса тем же API и выполняет чисто векторный поиск по одному индексу; rerank, фильтров, self-check и гибридного поиска нет.  
- **Компоновка промпта**: `compose_messages` берёт максимум 4 чанка и последовательно вставляет их в prompt без кластеризации и контроля токен-бюджета.  
- **Зависимости**: критическая зависимость от Qdrant и API эмбеддингов; при недоступности цепочка тихо деградирует к пустому контексту.

## Ограничения и проблемы

- Нет гарантий размера чанка и соответствия токен-бюджету; длинные параграфы могут «размазать» смысл.  
- Поиск зависит от одного векторного индекса — отсутствие BM25 и rerank ведёт к потере терминов и шуму.  
- Ингест блокирующий: синхронные вызовы эмбеддингового API на каждый чанк, нет батчинга и контроля частоты.  
- Отсутствие версионирования индекса и графика пересборки усложняет управление качеством.  
- Контекст в LLM не содержит цитат с путями/строками, нельзя сослаться на источник.  
- Нет механизма повторного поиска при нехватке фактов и сигналов об уверенности.

## Цели новой архитектуры

1. Построить гибридный retrieval (BM25 + dense) с регулярным инкрементальным обновлением индексов.  
2. Добавить многошаговый поисковый пайплайн: HyDE/переформулирование, RRF, кросс-энкодер, MMR.  
3. Контролировать токен-бюджет и формировать промпт из сжатых резюме кластеров и точных цитат.  
4. Обеспечить self-check и повторный поиск при недостатке доказательств.  
5. Минимизировать внешние зависимости: локальные индексы (Faiss/Chroma + BM25), внешняя API-модель только для генерации ответа.  
6. Улучшить обсервабилити: метрики hit-rate, latency, логи ключевых стадий.

## Проектирование целевой системы

### Компоненты

- `Chunker`: токен-бейз нарезка (500–800 токенов) с перекрытием 15 %, извлечение путей, номеров строк, SHA-версии.  
- `MetadataStore`: SQLite-таблицы `documents`, `chunks`, `index_versions` для связи чанков, ревизий и индексов.  
- `DenseIndexManager`: хранение эмбеддингов в Faiss (IndexFlatIP) с on-disk стореджем через Chroma; поддержка батчинга и инкрементальных обновлений.  
- `BM25IndexManager`: локальный `rank_bm25` + сериализация токенизированных коллекций в `data/indices/bm25`.  
- `HybridRetriever`: orchestrator, выполняющий HyDE/переформулирование, гибридный поиск, RRF-соединение, rerank кросс-энкодером (модель `BAAI/bge-reranker-large`) и MMR-диверсификацию.  
- `ContextBuilder`: кластеризация результатів (MiniBatchKMeans), map-reduce резюме (через локальный summarizer `bge-small` + LLM), подбор цитат с указанием пути и диапазона строк, контроль лимита токенов (токенизатор `tiktoken`).

### Модели и зависимости

- **Dense-эмбеддинги**: `BAAI/bge-small-en-v1.5` (быстрые, 384-d), опционально `int8` квантование.  
- **BM25**: `rank-bm25` с pre-tokenization через `spacy` (`en_core_web_sm`) или `nltk`.  
- **HyDE**: использование существующего `chat_completion` с системным промптом «генерируй аннотацию документа по запросу».  
- **Rerank**: `BAAI/bge-reranker-large` (768-d cross-encoder); fallback — `cross-encoder/ms-marco-MiniLM-L-4-v2`.  
- **Кластеризация**: `scikit-learn`.  
- **Токенизация**: `tiktoken` для моделей семейства GPT, `huggingface/tokenizers` для fallback.

### Потоки данных

1. **Ингест**: файл → `Chunker` → очищенный текст + диапазон строк → `MetadataStore` → батч эмбеддингов → запись в `DenseIndexManager` и `BM25IndexManager`.  
2. **Ежемесячный пересчёт**: cron вызывает `rebuild_indices`, который перечитывает `chunks`, строит новые индексы, атомарно заменяет директории.  
3. **Онлайн-запрос**: пользовательский текст → HyDE генерация → эмбеддинг запроса → dense search (top-50) + BM25 (top-50) → RRF → rerank top-40 → MMR top-12 → кластеризация → резюме + цитаты → финальный контекст.  
4. **Self-check**: если доверие < порога (например, средний score < 0.25), повторить шаг 3 с уточнённым запросом (переформулировка LLM) до 2 итераций.  
5. **Логирование**: для каждого запроса сохранять latency стадий, распределение скорингов, идентификаторы выбранных цитат.

### Параметры и конфигурация

- Новые ключи `Settings`: `indices_path`, `bm25_min_df`, `dense_top_k`, `bm25_top_k`, `rerank_model`, `hyde_prompt`, `mmr_lambda`, `max_context_tokens`.  
- Управление версиями индексов через `index_versions` (fields: name, version, built_at, source_hash).  
- Параметры self-check и пороги уверенности конфигурируемые.

### Риски и смягчение

- **Производительность**: Faiss требует подкачку, поэтому используем `IndexFlatIP` с возможностью переключения на HNSW при росте объёма.  
- **Память**: BGE-эмбеддинги 384-d (~1.5 KB/ч), контролируем лимитом чанков и квантованием.  
- **Качество HyDE**: ограничиваем генерацию 2–3 абзацами, валидируем по токенам.  
- **Совместимость**: сохраним старый API `retrieve_for_text`, добавим фича-флаги через Settings.  
- **Тестирование**: подготовим фикстуры из тестовых документов, метрики precision@3, MRR.


